{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d1839f-f8b3-408e-a79a-61eaa71079a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-aws langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f8a4d39-f0c9-4773-a769-a68e439875c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc4fbe3-a516-4689-9eb7-fe4cb846e268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d176fc-09e4-4092-b801-3b1b57b3aaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example tweets and their metadata\n",
    "raw_tweets = [\n",
    "    {\"id\": \"001\", \"text\": \"It's been a week since the Sparks Valley fire roared to life, and it's now the largest wildfire in state history.\", \"date\": \"03-04-2024\"},\n",
    "    {\"id\": \"002\", \"text\": \"Video of the flames destroying my neighborhood. **** that fire! #firessuck\", \"date\": \"03-04-2024\"},\n",
    "    {\"id\": \"003\", \"text\": \"I blame Biden for the #sparksvalleyfire response\", \"date\": \"02-29-2024\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98330b7-0e8e-4566-ab76-edd8e7b786ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field, Extra\n",
    "\n",
    "class Tweet(BaseModel):\n",
    "    isPolitical: bool = Field(description=\"Whether the tweet is political\")\n",
    "    isOffensive: bool = Field(description=\"Whether the tweet is offensive\")\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid  # Forbid extra fields not defined in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c2b54fd-d77c-4db2-a4e1-0116b3d8610c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's been a week since the Sparks Valley fire roared to life, and it's now the largest wildfire in state history.\n",
      "{'isPolitical': False, 'isOffensive': False}\n",
      "------\n",
      "Video of the flames destroying my neighborhood. **** that fire! #firessuck\n",
      "{'isPolitical': False, 'isOffensive': True}\n",
      "------\n",
      "I blame Biden for the #sparksvalleyfire response\n",
      "{'isPolitical': True, 'isOffensive': False}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# This example uses Langchain as a basis for interacting with a \n",
    "# local Ollama model but conceptually applies to any LLM.\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic.v1 import ValidationError\n",
    "\n",
    "screened_tweets = []\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Tweet)\n",
    "\n",
    "def check_tweet(tweet):\n",
    "    # Extract tweet text\n",
    "    tweet_text = tweet['text']\n",
    "    print(tweet_text)\n",
    "\n",
    "    # Define a prompt template for assessing tweet content\n",
    "    # and include the formatting instructions \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Assess whether the tweet contains references to politicians or political parties. \n",
    "        Assess if the tweet contains offensive language. \n",
    "        {format_instructions}\n",
    "        \n",
    "        {tweet_text}\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Construct a Langchain Chain to connect the prompt template with the LLM and Pydantic parser\n",
    "    chain = prompt | llm | parser\n",
    "    result = chain.invoke({\"tweet_text\": tweet_text})\n",
    "    \n",
    "    print(result)\n",
    "    print(\"------\")\n",
    "\n",
    "for tweet in raw_tweets:\n",
    "    check_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77da05-d618-409b-830b-4f4f48990742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
